"""
BLOC 1 - CompÃ©tence C1.7, C1.8, C1.9
Pipeline ETL (Extract, Transform, Load)

Objectif: AgrÃ©ger diffÃ©rentes sources de donnÃ©es, les nettoyer et les charger
dans la base de donnÃ©es PostgreSQL de maniÃ¨re optimisÃ©e.
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import json
from pathlib import Path
from typing import Dict, List, Tuple
import warnings
from faker import Faker
import random
warnings.filterwarnings('ignore')

fake = Faker('fr_FR')
Faker.seed(42)
random.seed(42)
np.random.seed(42)


class ETLPipeline:
    """
    Pipeline ETL complet pour le systÃ¨me de dÃ©tection de fraude.
    
    Couvre les compÃ©tences:
    - C1.7: AgrÃ©ger diffÃ©rentes sources de donnÃ©es (CSV, JSON, Excel, API)
    - C1.8: Analyser et corriger anomalies/valeurs manquantes
    - C1.9: Alimenter base de donnÃ©es avec donnÃ©es nettoyÃ©es
    """
    
    def __init__(self):
        """Initialisation du pipeline ETL"""
        self.data_sources = {}
        self.cleaned_data = {}
        self.stats_nettoyage = {}
        
    def generer_donnees_test(self, nb_transactions: int = 10000):
        """
        GÃ©nÃ¨re des donnÃ©es de test rÃ©alistes pour la dÃ©mo
        (En production: connexion aux vraies sources)
        
        Args:
            nb_transactions: Nombre de transactions Ã  gÃ©nÃ©rer
        """
        print("=" * 80)
        print("ğŸ­ GÃ‰NÃ‰RATION DONNÃ‰ES DE TEST")
        print("=" * 80)
        
        # 1. Transactions CSV
        print("\nğŸ“„ GÃ©nÃ©ration fichier: transactions.csv")
        
        transactions = []
        for i in range(nb_transactions):
            # 0.25% de fraudes (rÃ©aliste)
            is_fraud = random.random() < 0.0025
            
            montant = random.uniform(5, 2000) if not is_fraud else random.uniform(500, 5000)
            
            # Valeurs manquantes intentionnelles (3%)
            ville = fake.city() if random.random() > 0.03 else None
            merchant_cat = random.choice([
                'SUPERMARCHE', 'RESTAURANT', 'ESSENCE', 'VETEMENTS', 
                'PHARMACIE', 'LOISIRS', None
            ])
            
            trans = {
                'transaction_id': f'TRX_{i:08d}',
                'client_id': f'CLIENT_{random.randint(1, 5000):06d}',
                'montant': round(montant, 2),
                'devise': 'EUR',
                'date_heure': fake.date_time_between(
                    start_date='-30d', 
                    end_date='now'
                ).isoformat(),
                'type_transaction': random.choice(['PAIEMENT', 'RETRAIT', 'VIREMENT']),
                'type_carte': random.choice(['DEBIT', 'CREDIT', 'PREPAYEE']),
                'merchant_id': f'MERCHANT_{random.randint(1, 500):05d}',
                'merchant_category': merchant_cat,
                'pays': 'FR',
                'ville': ville,
                'latitude': round(random.uniform(42, 51), 6) if ville else None,
                'longitude': round(random.uniform(-4, 8), 6) if ville else None,
                'canal': random.choice(['WEB', 'MOBILE', 'ATM', 'POS']),
                'is_fraud': is_fraud
            }
            transactions.append(trans)
        
        df_transactions = pd.DataFrame(transactions)
        Path('data/raw').mkdir(parents=True, exist_ok=True)
        df_transactions.to_csv('data/raw/transactions.csv', index=False)
        print(f"   âœ… {len(df_transactions):,} transactions gÃ©nÃ©rÃ©es")
        print(f"   ğŸ“Š Taux fraude: {df_transactions['is_fraud'].sum() / len(df_transactions) * 100:.2f}%")
        
        # 2. DonnÃ©es clients Excel
        print("\nğŸ“Š GÃ©nÃ©ration fichier: clients.xlsx")
        
        clients_uniques = df_transactions['client_id'].unique()
        clients = []
        for client_id in clients_uniques:
            # Valeurs manquantes intentionnelles (11%)
            score_credit = random.randint(300, 850) if random.random() > 0.11 else None
            
            client = {
                'client_id': client_id,
                'age': random.randint(18, 80),
                'sexe': random.choice(['M', 'F', 'X']),
                'code_postal': fake.postcode()[:5],
                'anciennete_mois': random.randint(1, 120),
                'nb_produits': random.randint(1, 5),
                'revenu_annuel_tranche': random.choice([
                    '0-20K', '20-40K', '40-60K', '60-100K', '>100K'
                ]),
                'score_credit': score_credit,
                'date_derniere_fraude': fake.date_between(
                    start_date='-2y', 
                    end_date='today'
                ) if random.random() < 0.05 else None
            }
            clients.append(client)
        
        df_clients = pd.DataFrame(clients)
        df_clients.to_excel('data/raw/clients.xlsx', index=False)
        print(f"   âœ… {len(df_clients):,} clients gÃ©nÃ©rÃ©s")
        
        # 3. Logs comportementaux JSON
        print("\nğŸ“± GÃ©nÃ©ration fichier: logs_comportement.json")
        
        logs = []
        for i in range(nb_transactions * 5):  # 5 logs par transaction en moyenne
            log = {
                'session_id': f'SESSION_{i:08d}',
                'client_id': random.choice(clients_uniques),
                'timestamp': fake.date_time_between(
                    start_date='-30d', 
                    end_date='now'
                ).isoformat(),
                'action': random.choice([
                    'login', 'navigation_compte', 'consultation_solde', 
                    'transaction', 'logout'
                ]),
                'device_type': random.choice(['mobile', 'desktop', 'tablet']),
                'os': random.choice(['iOS', 'Android', 'Windows', 'macOS']),
                'browser': random.choice(['Chrome', 'Safari', 'Firefox', 'Edge']),
                'ip_address': fake.ipv4(),
                'duree_session_sec': random.randint(30, 3600)
            }
            logs.append(log)
        
        with open('data/raw/logs_comportement.json', 'w') as f:
            json.dump(logs, f, indent=2)
        print(f"   âœ… {len(logs):,} logs comportementaux gÃ©nÃ©rÃ©s")
        
        print("\nâœ… GÃ©nÃ©ration donnÃ©es de test terminÃ©e\n")
    
    def extraire_donnees(self) -> Dict[str, pd.DataFrame]:
        """
        C1.7: Extraction des donnÃ©es depuis diffÃ©rentes sources
        
        Returns:
            Dict contenant les DataFrames de chaque source
        """
        print("=" * 80)
        print("ğŸ“¥ EXTRACTION DES DONNÃ‰ES (ETL - Extract)")
        print("=" * 80)
        
        # Source 1: CSV
        print("\nğŸ“„ Extraction: transactions.csv")
        df_transactions = pd.read_csv('data/raw/transactions.csv')
        print(f"   âœ… {len(df_transactions):,} transactions extraites")
        print(f"   ğŸ“Š Colonnes: {list(df_transactions.columns)}")
        self.data_sources['transactions'] = df_transactions
        
        # Source 2: Excel
        print("\nğŸ“Š Extraction: clients.xlsx")
        df_clients = pd.read_excel('data/raw/clients.xlsx')
        print(f"   âœ… {len(df_clients):,} clients extraits")
        print(f"   ğŸ“Š Colonnes: {list(df_clients.columns)}")
        self.data_sources['clients'] = df_clients
        
        # Source 3: JSON
        print("\nğŸ“± Extraction: logs_comportement.json")
        with open('data/raw/logs_comportement.json', 'r') as f:
            logs = json.load(f)
        df_logs = pd.DataFrame(logs)
        print(f"   âœ… {len(df_logs):,} logs extraits")
        print(f"   ğŸ“Š Colonnes: {list(df_logs.columns)}")
        self.data_sources['logs'] = df_logs
        
        print("\nâœ… Extraction terminÃ©e: 3 sources chargÃ©es\n")
        return self.data_sources
    
    def analyser_qualite_donnees(self, df: pd.DataFrame, nom_source: str) -> Dict:
        """
        C1.8: Analyse de la qualitÃ© des donnÃ©es
        
        Args:
            df: DataFrame Ã  analyser
            nom_source: Nom de la source de donnÃ©es
            
        Returns:
            Dict avec statistiques de qualitÃ©
        """
        stats = {
            'nom': nom_source,
            'nb_lignes': len(df),
            'nb_colonnes': len(df.columns),
            'valeurs_manquantes': {},
            'doublons': 0,
            'anomalies': {}
        }
        
        # Valeurs manquantes
        for col in df.columns:
            nb_nan = df[col].isna().sum()
            if nb_nan > 0:
                pct = (nb_nan / len(df)) * 100
                stats['valeurs_manquantes'][col] = {
                    'count': int(nb_nan),
                    'pct': round(pct, 2)
                }
        
        # Doublons
        stats['doublons'] = int(df.duplicated().sum())
        
        # Anomalies spÃ©cifiques
        if 'montant' in df.columns:
            stats['anomalies']['montants_negatifs'] = int((df['montant'] < 0).sum())
            stats['anomalies']['montants_extremes'] = int((df['montant'] > 10000).sum())
        
        if 'latitude' in df.columns:
            invalides = ((df['latitude'] < -90) | (df['latitude'] > 90)).sum()
            stats['anomalies']['coordonnees_invalides'] = int(invalides)
        
        return stats
    
    def nettoyer_donnees(self) -> Dict[str, pd.DataFrame]:
        """
        C1.8: Nettoyage et correction des donnÃ©es
        
        Returns:
            Dict avec DataFrames nettoyÃ©s
        """
        print("=" * 80)
        print("ğŸ§¹ NETTOYAGE DES DONNÃ‰ES (ETL - Transform)")
        print("=" * 80)
        
        # 1. Nettoyage transactions
        print("\nğŸ“„ Nettoyage: Transactions")
        df_trans = self.data_sources['transactions'].copy()
        
        # Analyse qualitÃ© avant
        stats_avant = self.analyser_qualite_donnees(df_trans, 'transactions')
        print(f"   ğŸ“Š Avant nettoyage:")
        print(f"      â€¢ Lignes: {stats_avant['nb_lignes']:,}")
        print(f"      â€¢ Valeurs manquantes: {sum(v['count'] for v in stats_avant['valeurs_manquantes'].values())}")
        print(f"      â€¢ Doublons: {stats_avant['doublons']}")
        
        # Correction valeurs manquantes QUANTITATIVES: mÃ©diane
        if 'latitude' in df_trans.columns:
            avant_nan = df_trans['latitude'].isna().sum()
            df_trans['latitude'].fillna(df_trans['latitude'].median(), inplace=True)
            df_trans['longitude'].fillna(df_trans['longitude'].median(), inplace=True)
            print(f"      âœ… Latitude/Longitude: {avant_nan} valeurs imputÃ©es (mÃ©diane)")
        
        # Correction valeurs manquantes QUALITATIVES: mode ou 'INCONNU'
        if 'ville' in df_trans.columns:
            avant_nan = df_trans['ville'].isna().sum()
            df_trans['ville'].fillna('VILLE_INCONNUE', inplace=True)
            print(f"      âœ… Ville: {avant_nan} valeurs imputÃ©es ('VILLE_INCONNUE')")
        
        if 'merchant_category' in df_trans.columns:
            avant_nan = df_trans['merchant_category'].isna().sum()
            # Imputation par mode (catÃ©gorie la plus frÃ©quente)
            mode_cat = df_trans['merchant_category'].mode()[0] if not df_trans['merchant_category'].mode().empty else 'AUTRE'
            df_trans['merchant_category'].fillna(mode_cat, inplace=True)
            print(f"      âœ… Merchant category: {avant_nan} valeurs imputÃ©es (mode='{mode_cat}')")
        
        # Suppression doublons
        avant_dupl = len(df_trans)
        df_trans.drop_duplicates(subset=['transaction_id'], keep='first', inplace=True)
        nb_dupl_suppr = avant_dupl - len(df_trans)
        if nb_dupl_suppr > 0:
            print(f"      âœ… Doublons: {nb_dupl_suppr} lignes supprimÃ©es")
        
        # Correction anomalies montants
        avant_negatifs = (df_trans['montant'] < 0).sum()
        df_trans = df_trans[df_trans['montant'] >= 0]
        if avant_negatifs > 0:
            print(f"      âœ… Montants nÃ©gatifs: {avant_negatifs} transactions supprimÃ©es")
        
        # Conversion types
        df_trans['date_heure'] = pd.to_datetime(df_trans['date_heure'])
        print(f"      âœ… Type date_heure converti en datetime")
        
        stats_apres = self.analyser_qualite_donnees(df_trans, 'transactions')
        print(f"   ğŸ“Š AprÃ¨s nettoyage:")
        print(f"      â€¢ Lignes: {stats_apres['nb_lignes']:,}")
        print(f"      â€¢ Valeurs manquantes: {sum(v['count'] for v in stats_apres['valeurs_manquantes'].values())}")
        
        self.cleaned_data['transactions'] = df_trans
        self.stats_nettoyage['transactions'] = {
            'avant': stats_avant,
            'apres': stats_apres
        }
        
        # 2. Nettoyage clients
        print("\nğŸ“Š Nettoyage: Clients")
        df_clients = self.data_sources['clients'].copy()
        
        stats_avant = self.analyser_qualite_donnees(df_clients, 'clients')
        print(f"   ğŸ“Š Avant nettoyage:")
        print(f"      â€¢ Lignes: {stats_avant['nb_lignes']:,}")
        print(f"      â€¢ Valeurs manquantes: {sum(v['count'] for v in stats_avant['valeurs_manquantes'].values())}")
        
        # Score crÃ©dit: imputation mÃ©diane
        if 'score_credit' in df_clients.columns:
            avant_nan = df_clients['score_credit'].isna().sum()
            mediane = df_clients['score_credit'].median()
            df_clients['score_credit'].fillna(mediane, inplace=True)
            print(f"      âœ… Score crÃ©dit: {avant_nan} valeurs imputÃ©es (mÃ©diane={mediane:.0f})")
        
        # Date derniÃ¨re fraude: garder NULL si pas de fraude
        # (NULL a du sens ici)
        
        stats_apres = self.analyser_qualite_donnees(df_clients, 'clients')
        print(f"   ğŸ“Š AprÃ¨s nettoyage:")
        print(f"      â€¢ Valeurs manquantes: {sum(v['count'] for v in stats_apres['valeurs_manquantes'].values())}")
        
        self.cleaned_data['clients'] = df_clients
        self.stats_nettoyage['clients'] = {
            'avant': stats_avant,
            'apres': stats_apres
        }
        
        # 3. Nettoyage logs
        print("\nğŸ“± Nettoyage: Logs comportementaux")
        df_logs = self.data_sources['logs'].copy()
        
        # Conversion timestamp
        df_logs['timestamp'] = pd.to_datetime(df_logs['timestamp'])
        
        # Suppression doublons stricts
        avant_dupl = len(df_logs)
        df_logs.drop_duplicates(inplace=True)
        nb_dupl_suppr = avant_dupl - len(df_logs)
        if nb_dupl_suppr > 0:
            print(f"      âœ… Doublons: {nb_dupl_suppr} logs supprimÃ©s")
        
        self.cleaned_data['logs'] = df_logs
        
        print("\nâœ… Nettoyage terminÃ©: DonnÃ©es prÃªtes pour chargement\n")
        return self.cleaned_data
    
    def agrÃ©ger_donnÃ©es(self) -> pd.DataFrame:
        """
        C1.7: AgrÃ©gation des diffÃ©rentes sources
        Jointure transactions + clients + features logs
        
        Returns:
            DataFrame agrÃ©gÃ©
        """
        print("=" * 80)
        print("ğŸ”— AGRÃ‰GATION DES SOURCES DE DONNÃ‰ES")
        print("=" * 80)
        
        df_trans = self.cleaned_data['transactions']
        df_clients = self.cleaned_data['clients']
        df_logs = self.cleaned_data['logs']
        
        # Feature engineering sur logs: nb sessions par client
        print("\nğŸ”§ CrÃ©ation features depuis logs...")
        logs_agg = df_logs.groupby('client_id').agg({
            'session_id': 'count',
            'duree_session_sec': 'mean'
        }).reset_index()
        logs_agg.columns = ['client_id', 'nb_sessions', 'duree_moy_session_sec']
        print(f"   âœ… Features crÃ©Ã©es: nb_sessions, duree_moy_session_sec")
        
        # Jointure transactions + clients
        print("\nğŸ”— Jointure: Transactions LEFT JOIN Clients")
        df_merged = df_trans.merge(
            df_clients,
            on='client_id',
            how='left',
            suffixes=('', '_client')
        )
        print(f"   âœ… {len(df_merged):,} lignes aprÃ¨s jointure")
        
        # Jointure avec features logs
        print("\nğŸ”— Jointure: + Features Logs")
        df_merged = df_merged.merge(
            logs_agg,
            on='client_id',
            how='left'
        )
        
        # Imputation valeurs manquantes features logs (clients sans logs)
        df_merged['nb_sessions'].fillna(0, inplace=True)
        df_merged['duree_moy_session_sec'].fillna(0, inplace=True)
        
        print(f"   âœ… {len(df_merged):,} lignes finales")
        print(f"   ğŸ“Š Nombre total de colonnes: {len(df_merged.columns)}")
        
        print(f"\n   ğŸ“‹ AperÃ§u colonnes finales:")
        for i, col in enumerate(df_merged.columns, 1):
            print(f"      {i:2d}. {col}")
        
        self.cleaned_data['dataset_final'] = df_merged
        
        print("\nâœ… AgrÃ©gation terminÃ©e\n")
        return df_merged
    
    def charger_donnees(self, simulate: bool = True) -> Dict:
        """
        C1.9: Chargement des donnÃ©es dans PostgreSQL
        
        Args:
            simulate: Si True, simule le chargement (sans vraie BDD)
            
        Returns:
            Dict avec statistiques de chargement
        """
        print("=" * 80)
        print("ğŸ’¾ CHARGEMENT DES DONNÃ‰ES (ETL - Load)")
        print("=" * 80)
        
        stats_chargement = {}
        
        for table_name, df in self.cleaned_data.items():
            print(f"\nğŸ“¤ Chargement table: {table_name}")
            
            if simulate:
                # Simulation: sauvegarde en CSV dans data/processed
                Path('data/processed').mkdir(parents=True, exist_ok=True)
                output_path = f'data/processed/{table_name}_clean.csv'
                df.to_csv(output_path, index=False)
                
                print(f"   â„¹ï¸  Mode simulation: donnÃ©es sauvÃ©es en {output_path}")
                print(f"   ğŸ“Š Lignes: {len(df):,}")
                print(f"   ğŸ“Š Colonnes: {len(df.columns)}")
                
                # VÃ©rification intÃ©gritÃ© (Ã©quivalent COUNT(*) en SQL)
                nb_lignes_source = len(df)
                nb_lignes_fichier = len(pd.read_csv(output_path))
                
                if nb_lignes_source == nb_lignes_fichier:
                    print(f"   âœ… VÃ©rification intÃ©gritÃ©: {nb_lignes_source:,} = {nb_lignes_fichier:,}")
                else:
                    print(f"   âŒ ERREUR: {nb_lignes_source} â‰  {nb_lignes_fichier}")
                
                stats_chargement[table_name] = {
                    'lignes': nb_lignes_source,
                    'colonnes': len(df.columns),
                    'fichier': output_path,
                    'status': 'OK'
                }
            else:
                # Chargement rÃ©el PostgreSQL (nÃ©cessite connexion)
                # from sqlalchemy import create_engine
                # engine = create_engine('postgresql://...')
                # df.to_sql(table_name, engine, if_exists='append', index=False)
                print("   âš ï¸  Chargement PostgreSQL nÃ©cessite connexion configurÃ©e")
        
        print(f"\n{'='*80}")
        print("âœ… CHARGEMENT TERMINÃ‰")
        print(f"{'='*80}\n")
        
        return stats_chargement
    
    def generer_rapport_etl(self) -> str:
        """
        GÃ©nÃ¨re un rapport complet du pipeline ETL
        
        Returns:
            Chemin du fichier rapport
        """
        rapport = {
            'date_execution': datetime.now().isoformat(),
            'etapes': ['Extract', 'Transform', 'Load'],
            'sources_extraites': list(self.data_sources.keys()),
            'stats_nettoyage': self.stats_nettoyage,
            'donnees_finales': {
                name: {
                    'nb_lignes': len(df),
                    'nb_colonnes': len(df.columns)
                }
                for name, df in self.cleaned_data.items()
            }
        }
        
        output_path = 'data/processed/rapport_etl.json'
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(rapport, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ Rapport ETL gÃ©nÃ©rÃ©: {output_path}")
        return output_path


def main():
    """Point d'entrÃ©e principal"""
    print("\n" + "ğŸš€"*40)
    print("BLOC 1 - PIPELINE ETL COMPLET")
    print("CompÃ©tences C1.7, C1.8, C1.9")
    print("ğŸš€"*40 + "\n")
    
    # Initialisation
    etl = ETLPipeline()
    
    # GÃ©nÃ©ration donnÃ©es test
    print("\nğŸ“‹ Ã‰tape 0: GÃ©nÃ©ration donnÃ©es de test")
    etl.generer_donnees_test(nb_transactions=10000)
    
    # C1.7: Extraction
    print("\nğŸ“‹ Ã‰tape 1: Extraction (Extract)")
    etl.extraire_donnees()
    
    # C1.8: Nettoyage (Transform)
    print("\nğŸ“‹ Ã‰tape 2: Nettoyage (Transform)")
    etl.nettoyer_donnees()
    
    # C1.7: AgrÃ©gation
    print("\nğŸ“‹ Ã‰tape 3: AgrÃ©gation sources")
    etl.agrÃ©ger_donnÃ©es()
    
    # C1.9: Chargement (Load)
    print("\nğŸ“‹ Ã‰tape 4: Chargement (Load)")
    stats = etl.charger_donnees(simulate=True)
    
    # Rapport
    print("\nğŸ“‹ Ã‰tape 5: GÃ©nÃ©ration rapport ETL")
    etl.generer_rapport_etl()
    
    print(f"\n{'='*80}")
    print("âœ… PIPELINE ETL EXÃ‰CUTÃ‰ AVEC SUCCÃˆS")
    print(f"{'='*80}\n")
    
    # RÃ©sumÃ©
    print("ğŸ“Š RÃ‰SUMÃ‰:")
    for table, stat in stats.items():
        print(f"   â€¢ {table}: {stat['lignes']:,} lignes chargÃ©es")
    print()


if __name__ == "__main__":
    main()